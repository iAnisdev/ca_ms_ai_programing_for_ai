{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1 Problem 1: NASA APOD Data Retrieval and JSON File Processing (33 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the dotenv package to load the environment variables\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# load the environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# get the environment variables and store them in variables\n",
    "api_key = os.getenv('API_KEY')\n",
    "base_url = os.getenv('BASE_URL')\n",
    "\n",
    "\n",
    "# define the start and end date for the historical data\n",
    "start_date = '2020-01-01'\n",
    "end_date = '2020-12-31'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'date': '2024-11-19', 'title': 'Undulatus Clouds over Las Campanas Observatory', 'url': 'https://apod.nasa.gov/apod/image/2411/ParallelClouds_Beletsky_960.jpg', 'explanation': \"What's happening with these clouds?  While it may seem that these long and thin clouds are pointing toward the top of a hill, and that maybe a world-famous observatory is located there, only part of that is true. In terms of clouds, the formation is a chance superposition of impressively periodic undulating air currents in Earth's lower atmosphere. Undulatus, a type of Asperitas cloud, form at the peaks where the air is cool enough to cause the condensation of opaque water droplets.  The wide-angle nature of the panorama creates the illusion that the clouds converge over the hill.  In terms of land, there really is a world-famous observatory at the top of that peak: the Carnegie Science's Las Campanas Observatory in the Atacama Desert of Chile.  The two telescope domes visible are the 6.5-meter Magellan Telescopes.  The featured coincidental vista was a surprise but was captured by the phone of a quick-thinking photographer in late September.   Your Sky Surprise: What picture did APOD feature on your birthday? (post 1995)\", 'media_type': 'image'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from datetime import date , timedelta\n",
    "\n",
    "def get_apod_data(api_key, date):\n",
    "    url = f\"{base_url}/planetary/apod?api_key={api_key}&date={date}\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        # check if the response is successful and raise an exception incase of an error\n",
    "        response.raise_for_status()\n",
    "        # parse the JSON response\n",
    "        data = response.json()\n",
    "        return {\n",
    "            \"date\": data.get(\"date\"),\n",
    "            \"title\": data.get(\"title\"),\n",
    "            \"url\": data.get(\"url\"),\n",
    "            \"explanation\": data.get(\"explanation\"),\n",
    "            \"media_type\": data.get(\"media_type\")\n",
    "        }\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching data for {date}: {e}\")\n",
    "    except KeyError:\n",
    "        print(\"Unexpected response format\")\n",
    "\n",
    "# Get today's date  and fetch the data for today from the API to test the function\n",
    "today = date.today()\n",
    "today_data = get_apod_data(api_key, today)\n",
    "\n",
    "print(today_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.774s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x10e9f8c90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validate the response data by checking if the response is a dictionary and contains the expected keys and values \n",
    "import unittest\n",
    "\n",
    "class TestGetApodData(unittest.TestCase):\n",
    "    def test_get_apod_data(self):\n",
    "        data = get_apod_data(api_key, date.today())\n",
    "        self.assertIsInstance(data, dict)\n",
    "        self.assertIn(\"date\", data)\n",
    "        self.assertIn(\"title\", data)\n",
    "        self.assertIn(\"url\", data)\n",
    "        self.assertIn(\"explanation\", data)\n",
    "        self.assertIn(\"media_type\", data)\n",
    "    \n",
    "# Run the test\n",
    "unittest.main(argv=[''], exit=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Function to fetch APOD data for multiple dates within a range\n",
    "def fetch_multiple_apod_data(api_key, start_date, end_date):\n",
    "    current_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    \n",
    "    apod_data = []\n",
    "\n",
    "    while current_date <= end_date:\n",
    "        date_str = current_date.strftime(\"%Y-%m-%d\")\n",
    "        # Fetch data for the current date\n",
    "        data = get_apod_data(api_key, date_str)\n",
    "        if data:\n",
    "            print(f\"Fetched data for {date_str}\")\n",
    "            apod_data.append(data)\n",
    "        # Move to the next date\n",
    "        current_date += timedelta(days=1)\n",
    "        # Delay to respect API rate limits\n",
    "        time.sleep(1)  \n",
    "\n",
    "    return apod_data\n",
    "\n",
    "# apod_data = fetch_multiple_apod_data(api_key, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better approach the fetch the data for multiple dates using start_date and end_date as query parameters in the API URL\n",
    "\n",
    "def get_range_apod_data(api_key, start_date , end_date):\n",
    "    url = f\"{base_url}/planetary/apod?api_key={api_key}&start_date={start_date}&end_date={end_date}\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        formatted_data = []\n",
    "        for item in data:\n",
    "            formatted_data.append({\n",
    "                \"date\": item.get(\"date\"),\n",
    "                \"title\": item.get(\"title\"),\n",
    "                \"url\": item.get(\"url\"),\n",
    "                \"explanation\": item.get(\"explanation\"),\n",
    "                \"media_type\": item.get(\"media_type\")\n",
    "            })\n",
    "        return formatted_data\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching data for the range {start_date} to {end_date}: {e}\")\n",
    "    except KeyError:\n",
    "        print(\"Unexpected response format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def save_to_json(data, filename='apod_data.json'):\n",
    "    try:\n",
    "        # Verify if the data is a JSON-serializable list\n",
    "        if not isinstance(data, list):\n",
    "            raise ValueError(\"Data should be a list\")\n",
    "\n",
    "        # Check if the file exists\n",
    "        if not os.path.exists(filename):\n",
    "            # If the file does not exist, create it and add the data\n",
    "            with open(filename, 'w') as file:\n",
    "                json.dump(data, file, indent=4)\n",
    "                file.write(\"\\n\")\n",
    "            return\n",
    "        \n",
    "        # If the file exists, check if it contains any data\n",
    "        with open(filename, 'r+') as file:\n",
    "            if os.stat(filename).st_size == 0:\n",
    "                # If the file is empty, add the data directly\n",
    "                json.dump(data, file, indent=4)\n",
    "                file.write(\"\\n\")\n",
    "            else:\n",
    "                # If the file has data, load existing data, concatenate with new data\n",
    "                file.seek(0)\n",
    "                existing_data = json.load(file)\n",
    "                \n",
    "                # Ensure existing data is a list\n",
    "                if not isinstance(existing_data, list):\n",
    "                    raise ValueError(\"Existing file data is not in list format\")\n",
    "\n",
    "                # Concatenate existing data with the new data\n",
    "                updated_data = existing_data + data\n",
    "                \n",
    "                # Write the concatenated data back to the file\n",
    "                file.seek(0)\n",
    "                file.truncate(0)\n",
    "                json.dump(updated_data, file, indent=4)\n",
    "                file.write(\"\\n\")\n",
    "                \n",
    "    except ValueError as ve:\n",
    "        print(f\"Data validation error: {ve}\")\n",
    "    except IOError as e:\n",
    "        print(f\"Error writing to file {filename}: {e}\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error decoding JSON from file {filename}. Ensure the file format is correct.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched data for 365 dates\n"
     ]
    }
   ],
   "source": [
    "def fetch_and_save_apod_data(api_key, start_date, end_date):\n",
    "    # using the get_range_apod_data function for better performance and to avoid rate limits\n",
    "    range_data = get_range_apod_data(api_key, start_date, end_date)\n",
    "    # to test the loop function, uncomment the line below and comment the line above\n",
    "    # range_data = fetch_multiple_apod_data(api_key, start_date, end_date)\n",
    "    if range_data:\n",
    "        print(f\"Fetched data for {len(range_data)} dates\")\n",
    "        # Save the data to a JSON file with the default filename\n",
    "        save_to_json(range_data) \n",
    "\n",
    "\n",
    "fetch_and_save_apod_data(api_key, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2 Problem 2: JSON Data Reading,Looping,and Processing (27Marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_apod_data(filename='apod_data.json'):\n",
    "    try:\n",
    "        with open(filename, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        for entry in data:\n",
    "            print(f\"Date: {entry['date']}, Title: {entry['title']}\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {filename} was not found.\")\n",
    "    except PermissionError:\n",
    "        print(f\"Error: Permission denied when accessing {filename}.\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: The file {filename} is empty or contains invalid JSON.\")\n",
    "    return None\n",
    "\n",
    "# Read the data from the JSON file\n",
    "saved_data = read_apod_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 329\n",
      "Number of videos: 36\n",
      "Date with the longest explanation: 2020-08-31 (Length: 1572 characters)\n"
     ]
    }
   ],
   "source": [
    "def analyze_apod_media(data):\n",
    "    img_count = 0 # image count\n",
    "    video_count = 0 # video count\n",
    "    detail_explanation = {\"date\": None , \"length\": 0} # date and length of the explanation with default values of None and 0 respectively\n",
    "\n",
    "    for entry in data:\n",
    "        if entry['media_type'] == 'image':\n",
    "            img_count += 1\n",
    "        elif entry['media_type'] == 'video':\n",
    "            video_count += 1\n",
    "        \n",
    "        explanation_length = len(entry.get(\"explanation\", \"\"))\n",
    "        if explanation_length > detail_explanation['length']:\n",
    "            detail_explanation['date'] = entry['date']\n",
    "            detail_explanation['length'] = explanation_length\n",
    "\n",
    "    print(f\"Number of images: {img_count}\")\n",
    "    print(f\"Number of videos: {video_count}\")\n",
    "    print(f\"Date with the longest explanation: {detail_explanation['date']} (Length: {detail_explanation['length']} characters)\")\n",
    "\n",
    "\n",
    "analyze_apod_media(saved_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to apod_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# Extract data into csv file\n",
    "import csv\n",
    "\n",
    "def write_to_csv(data, filename='apod_summary.csv'):\n",
    "    try:\n",
    "        with open(filename, 'a', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            # Write headers if the file is empty\n",
    "            if csvfile.tell() == 0:\n",
    "                writer.writerow([\"date\", \"title\", \"media_type\", \"url\"])\n",
    "\n",
    "            # Write data entries\n",
    "            for entry in data:\n",
    "                writer.writerow([\n",
    "                    entry.get(\"date\", \"\"),\n",
    "                    entry.get(\"title\", \"\"),\n",
    "                    entry.get(\"media_type\", \"\"),\n",
    "                    entry.get(\"url\", \"\")\n",
    "                ])\n",
    "        print(f\"Data successfully written to {filename}\")\n",
    "    except IOError as e:\n",
    "        print(f\"Error writing to CSV file {filename}: {e}\")\n",
    "\n",
    "\n",
    "if saved_data:\n",
    "    write_to_csv(saved_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_array(rows=20, cols=5):\n",
    "    main_array = np.empty((0, 5), dtype=int) \n",
    "    while main_array.shape[0] < rows:\n",
    "\n",
    "        # Generate a random row\n",
    "        new_row = np.random.randint(10, 100, size=(1, cols))\n",
    "        \n",
    "        # Condition 1: Check if the sum of the row is even\n",
    "        if new_row.sum() % 2 == 0:\n",
    "\n",
    "            temp_array = np.vstack([main_array, new_row])\n",
    "\n",
    "            # Condition 2: Check if the sum of all values in the array is a multiple of 5\n",
    "            if temp_array.sum() % 5 == 0:\n",
    "                # If both conditions are met, update the main array\n",
    "                main_array =temp_array\n",
    "\n",
    "    return main_array\n",
    "\n",
    "\n",
    "array = create_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_array(array):\n",
    "    # Elements divisible by both 3 and 5\n",
    "    divisible_by_3_and_5 = array[(array % 3 == 0) & (array % 5 == 0)]\n",
    "    print(\"Elements divisible by both 3 and 5:\", divisible_by_3_and_5)\n",
    "\n",
    "    # Replace elements > 75 with the array mean\n",
    "    mean_value = array.mean()\n",
    "    print(\"Mean value:\", mean_value)\n",
    "    array[array > 75] = mean_value\n",
    "    print(\"Modified Array:\\n\", array)\n",
    "\n",
    "process_array(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical Operations\n",
    "mean = array.mean()\n",
    "std_dev = array.std()\n",
    "median = np.median(array)\n",
    "column_variance = array.var(axis=0)\n",
    "\n",
    "print(f\"Mean of array: {mean}\")\n",
    "print(f\"Standard deviation of array: {std_dev}\")\n",
    "print(f\"Median of array: {median}\")\n",
    "print(f\"Variance of each column: {column_variance}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
